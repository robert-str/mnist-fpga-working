{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f339fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Quantization imports\n",
        "from torch.quantization import QuantStub, DeQuantStub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1b32e2ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing MNIST datasets (torchvision)...\n",
            "Device: cpu\n",
            "Train batches: 235, Test batches: 40\n"
          ]
        }
      ],
      "source": [
        "# Prepare torchvision MNIST datasets and loaders\n",
        "print(\"Preparing MNIST datasets (torchvision)...\")\n",
        "\n",
        "# Quantization is typically done on CPU in PyTorch\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),            # [0,1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # standard MNIST normalization\n",
        "])\n",
        "\n",
        "# Adjust paths since we are in 2_hidden/\n",
        "train_dataset = datasets.MNIST(root=\"../data/MNIST\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"../data/MNIST\", train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "input_dim = 28 * 28\n",
        "num_classes = 10\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5328c846",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x200 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Preview a few images from the DataLoader\n",
        "np.set_printoptions(linewidth=200, threshold=784, suppress=True)\n",
        "\n",
        "examples, labels = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(1, 6, figsize=(10, 2))\n",
        "for i in range(6):\n",
        "    axes[i].imshow(examples[i, 0].numpy(), cmap='gray')\n",
        "    axes[i].set_title(int(labels[i]))\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "50a81516",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNISTTwoHiddenQuant(\n",
            "  (quant): QuantStub()\n",
            "  (net): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=784, out_features=16, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=16, out_features=10, bias=True)\n",
            "  )\n",
            "  (dequant): DeQuantStub()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define a neural network with 2 hidden layers for Quantization\n",
        "class MNISTTwoHiddenQuant(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim1: int, hidden_dim2: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.quant = QuantStub()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_dim, hidden_dim1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim2, num_classes)\n",
        "        )\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.net(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "hidden_dim1 = 16\n",
        "hidden_dim2 = 16\n",
        "model = MNISTTwoHiddenQuant(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "030cf2ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 [100/235] Loss: 1.1839\n",
            "Epoch 1 [200/235] Loss: 0.4339\n",
            "Epoch 1 training accuracy: 78.55%\n",
            "Epoch 1 test accuracy: 90.31%\n",
            "Epoch 2 [100/235] Loss: 0.3296\n",
            "Epoch 2 [200/235] Loss: 0.2976\n",
            "Epoch 2 training accuracy: 91.06%\n",
            "Epoch 2 test accuracy: 92.10%\n",
            "Epoch 3 [100/235] Loss: 0.2670\n",
            "Epoch 3 [200/235] Loss: 0.2673\n",
            "Epoch 3 training accuracy: 92.36%\n",
            "Epoch 3 test accuracy: 92.88%\n",
            "Epoch 4 [100/235] Loss: 0.2406\n",
            "Epoch 4 [200/235] Loss: 0.2376\n",
            "Epoch 4 training accuracy: 93.06%\n",
            "Epoch 4 test accuracy: 92.95%\n",
            "Epoch 5 [100/235] Loss: 0.2213\n",
            "Epoch 5 [200/235] Loss: 0.2182\n",
            "Epoch 5 training accuracy: 93.60%\n",
            "Epoch 5 test accuracy: 93.65%\n"
          ]
        }
      ],
      "source": [
        "# Train (Float32)\n",
        "num_epochs = 5  # Reduced for speed, but enough for convergence\n",
        "log_interval = 100\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader, start=1):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Epoch {epoch} [{batch_idx}/{len(train_loader)}] Loss: {running_loss/log_interval:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    train_acc = 100.0 * correct / total\n",
        "    train_accuracies.append(train_acc)\n",
        "    print(f\"Epoch {epoch} training accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    test_acc = 100.0 * correct / total\n",
        "    test_accuracies.append(test_acc)\n",
        "    print(f\"Epoch {epoch} test accuracy: {test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6a0fc05a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying quantization...\n",
            "Calibrating with training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_25556\\720622670.py:6: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.quantization.prepare(model, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantization complete.\n",
            "MNISTTwoHiddenQuant(\n",
            "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
            "  (net): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): QuantizedLinear(in_features=784, out_features=16, scale=0.5233570337295532, zero_point=55, qscheme=torch.per_channel_affine)\n",
            "    (2): ReLU()\n",
            "    (3): QuantizedLinear(in_features=16, out_features=16, scale=0.44000041484832764, zero_point=41, qscheme=torch.per_channel_affine)\n",
            "    (4): ReLU()\n",
            "    (5): QuantizedLinear(in_features=16, out_features=10, scale=0.3081290125846863, zero_point=67, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            "  (dequant): DeQuantize()\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_25556\\720622670.py:17: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.quantization.convert(model, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Apply Post-Training Static Quantization\n",
        "print(\"Applying quantization...\")\n",
        "model.eval()\n",
        "# Use 'fbgemm' for x86 or 'qnnpack' for ARM/mobile\n",
        "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# Calibrate\n",
        "print(\"Calibrating with training data...\")\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        if i > 100: break  # Use a subset for calibration\n",
        "        inputs = inputs.to(device)\n",
        "        model(inputs)\n",
        "\n",
        "# Convert to quantized model\n",
        "torch.quantization.convert(model, inplace=True)\n",
        "print(\"Quantization complete.\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a9252e57",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized Test Accuracy: 93.59%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate Quantized Model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "print(f\"Quantized Test Accuracy: {100.0 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5e4b4b6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved quantized weights to ../outputs\\mnist_2hidden_int8.pth\n"
          ]
        }
      ],
      "source": [
        "# Save Quantized Model\n",
        "os.makedirs(\"../outputs\", exist_ok=True)\n",
        "pth_path = os.path.join(\"../outputs\", \"mnist_2hidden_int8.pth\")\n",
        "torch.save(model.state_dict(), pth_path)\n",
        "print(f\"Saved quantized weights to {pth_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0536ed74",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 250x250 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: 7\n"
          ]
        }
      ],
      "source": [
        "# Predict digit from test2.png with INT8 model\n",
        "img_path = \"../test2.png\"\n",
        "\n",
        "if not os.path.exists(img_path):\n",
        "    print(\"test2.png not found\")\n",
        "else:\n",
        "    # Load, convert to grayscale, resize to 20x20\n",
        "    img_orig = Image.open(img_path).convert(\"L\")\n",
        "    img_20 = img_orig.resize((20, 20), resample=Image.BILINEAR)\n",
        "\n",
        "    # Create 28x28 black canvas and center the 20x20 digit\n",
        "    canvas = Image.new(\"L\", (28, 28), color=0)\n",
        "    offset = ((28 - 20) // 2, (28 - 20) // 2)\n",
        "    canvas.paste(img_20, offset)\n",
        "\n",
        "    # Show processed image\n",
        "    plt.figure(figsize=(2.5, 2.5))\n",
        "    plt.imshow(canvas, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Preprocess for model\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    tensor = to_tensor(canvas)\n",
        "    if tensor.mean().item() > 0.5:\n",
        "        tensor = 1.0 - tensor\n",
        "    normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "    tensor = normalize(tensor).unsqueeze(0)\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor)\n",
        "        pred = int(logits.argmax(dim=1).item())\n",
        "\n",
        "    plt.title(f\"Predicted (INT8): {pred}\")\n",
        "    plt.show()\n",
        "    print(f\"Prediction: {pred}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
